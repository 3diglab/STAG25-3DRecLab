{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"generative_ai_disabled":true,"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyPCZmR+eQ4Az2J8GCsMWMQl","include_colab_link": true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Intro\n","\n","*Speaker: S. Zuffi, S. Melzi*\n","\n","*Author: D. Baieri*\n","\n","In this demo, we'll see how to design step-by-step a 3D reconstruction method for a very challenging setting: drone-shot dolphin videos in open waters. The full version of this algorithm was developed as recent research in collaboration with the University of Zurich and presented at the CV4Animals Workshop at CVPR 2025.\n"],"metadata":{"id":"HAFFKtKEoEeY"}},{"cell_type":"markdown","source":["# 0. Setup\n","\n","Run the following cells to setup the environment. This takes a while, so do it ASAP!\n","\n","The notebook was designed to work inside Colab, but it should still work outside by changing file paths. If you're on Colab, remember to select a GPU runtime to use SAM-2."],"metadata":{"id":"ZHq2FzbdoI6e"}},{"cell_type":"code","source":["ROOT = '/content'    # change if outside Colab\n","!mkdir $ROOT/data/\n","!unzip -n $ROOT/stag_dolphins_assets.zip &> /dev/null\n","!pip install kaolin==0.18.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.9.0_cu126.html\n","!pip install huggingface_hub\n","!pip install git+https://github.com/mattloper/chumpy@9b045ff5d6588a24a0bab52c83f032e2ba433e17\n","!git clone https://github.com/facebookresearch/segment-anything-2.git\n","!cd segment-anything-2 && pip install -e ."],"metadata":{"id":"GNUrR5K7rKiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append(f'{ROOT}/segment-anything-2')\n","import torch\n","import random\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","\n","SEED = 123456789\n","DATA_DIR = f'{ROOT}/data/2024_08_20_DF2_S8_SW_TT_ELM-fb_23450-fe_23950'\n","MODEL_DIR = f'{ROOT}/model'\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.random.manual_seed(SEED)"],"metadata":{"id":"0XH0ooWKux2K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Preprocessing\n","\n","Our design starts with the input data. Inside the \"data\" folder, you will find a \"frames\" subfolder (containing video frames we want to reconstruct) and a \"metadata\" subfolder (which we will use later). First off, we need **masks** to tell us which pixels of each image contain the target object (the dolphin): to this end, we will use the powerful SAM-2 segmentation model."],"metadata":{"id":"30_9lBhpoMDh"}},{"cell_type":"code","source":["import pathlib\n","from PIL import Image\n","from sam2.sam2_video_predictor import SAM2VideoPredictor\n","\n","\n","def preprocessing():\n","\n","    predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n","    frames_dir = pathlib.Path(DATA_DIR) / 'frames'\n","    segment_dir = pathlib.Path(DATA_DIR) / 'segmentation'\n","    segment_dir.mkdir(exist_ok=True)\n","\n","    positive_points = np.array([[360, 245], [326, 287], [334, 293], [336, 280], [348, 251], [348, 262], [356, 259]])\n","    negative_points = np.array([[273, 272], [290, 229], [280, 245], [546, 125], [339, 180], [402, 311], [316, 261]])\n","\n","    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n","        state = predictor.init_state(str(frames_dir), async_loading_frames=True, offload_video_to_cpu=True)\n","\n","        predictor.add_new_points_or_box(\n","            inference_state=state, frame_idx=0, obj_id=1,\n","            points=np.concatenate([positive_points, negative_points], axis=0),\n","            labels=np.concatenate([np.ones(positive_points.shape[0]), np.zeros(negative_points.shape[0])], axis=0)\n","        )\n","\n","        # propagate the prompts to get masklets throughout the video\n","        t = 0\n","        for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n","            frame = 255 * np.uint8(np.transpose((masks[0] > 0).float().cpu().numpy(), [1, 2, 0]).repeat(3, axis=-1))\n","            Image.fromarray(frame).save(segment_dir / (f'{str(t).zfill(6)}.png'))\n","            t += 1\n","\n","if not (pathlib.Path(DATA_DIR) / 'segmentation').exists():\n","    preprocessing()"],"metadata":{"id":"wp0Af4U416OS","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's package all our data in a single Dataset class to use later in our reconstruction algorithm:"],"metadata":{"id":"uqS27YrL5ojw"}},{"cell_type":"code","source":["CAMERA_FOCAL = 12.29 #mm\n","SENSOR_WIDTH = 17.27 #mm\n","\n","import pathlib\n","import torchvision.transforms.functional as FV\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","\n","class SegmentedVideo(Dataset):\n","\n","    def __init__(self, data_dir: pathlib.Path, fps: int):\n","        super(SegmentedVideo, self).__init__()\n","        self.frames_original = []\n","        self.frames_segmented = []\n","        self.segment_centers = []\n","        for orig, segm in zip(sorted((data_dir / 'frames').iterdir()), sorted((data_dir / 'segmentation').iterdir())):\n","            orig_img, segm_img = Image.open(orig), Image.open(segm)\n","            self.frames_original.append(FV.pil_to_tensor(orig_img).permute(1, 2, 0) / 255.)  # (H, W)\n","            self.frames_segmented.append(FV.pil_to_tensor(FV.to_grayscale(segm_img)).permute(1, 2, 0) / 255.)  # (H, W)\n","            self.segment_centers.append(self.frames_segmented[-1].nonzero()[:, [1, 0]].float().mean(dim=0))  # (W, H)\n","        self.data_resolution = self.frames_original[0].shape[1::-1]  # (W, H)\n","        self.center_normalization = torch.tensor([self.data_resolution[0], self.data_resolution[1]])\n","\n","        self.fps = fps\n","\n","        # Load altitudes and store per-camera information\n","        altitudes = torch.from_numpy(np.load(data_dir / 'metadata' / 'altitude.npy')).float() / 100.\n","        altitudes = altitudes[50:250]  # filter specific frames out of 500, selected for this experiment\n","        self.cam_eye = torch.zeros(len(self.frames_original), 3, dtype=torch.float)\n","        self.cam_eye[:, 1] = altitudes\n","        self.cam_at = torch.zeros(3, dtype=torch.float)\n","        self.cam_up = torch.tensor([0, 0, 1], dtype=torch.float)\n","\n","    def __getitem__(self, idx):\n","        c = (self.segment_centers[idx] / (self.center_normalization / 2.)) - 1.0\n","        return {'original': self.frames_original[idx],\n","                'segmented': self.frames_segmented[idx],\n","                'segm_center': torch.stack([c[0], -c[1]], dim=0),\n","                't': idx,\n","                'cam_eye': self.cam_eye[idx],\n","                'cam_at': self.cam_at,\n","                'cam_up': self.cam_up,\n","                'cam_width': self.data_resolution[0],\n","                'cam_height': self.data_resolution[1]}\n","\n","    def __len__(self):\n","        return len(self.frames_original)\n","\n","def collate_batch(frames):\n","    return {\n","        'original': torch.stack([f['original'] for f in frames], dim=0),\n","        'segmented': torch.stack([f['segmented'] for f in frames], dim=0),\n","        'segm_center': torch.stack([f['segm_center'] for f in frames], dim=0),\n","        't': torch.tensor([f['t'] for f in frames], dtype=torch.long),\n","        'cam': kal.render.camera.Camera.from_args(\n","            eye=torch.stack([f['cam_eye'] for f in frames], dim=0),\n","            at=torch.stack([f['cam_at'] for f in frames], dim=0),\n","            up=torch.stack([f['cam_up'] for f in frames], dim=0),\n","            fov=2 * math.atan(SENSOR_WIDTH / (2 * CAMERA_FOCAL)),\n","            fov_direction=CameraFOV.HORIZONTAL, width=frames[0]['cam_width'], height=frames[0]['cam_height'],\n","            near=1e-2, far=1e2,\n","            dtype=torch.float32,\n","        )\n","    }\n","\n","dataset = SegmentedVideo(pathlib.Path(DATA_DIR), 50)"],"metadata":{"id":"eQGvqUj35vuB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 1\n","\n","Query the dataset object at various indices. Then, use the outputs to visualize the original frames and the SAM-2 segmentations, to ensure that the input data is good to go for the rest of the pipeline."],"metadata":{"id":"MbIMfqF41Z1c"}},{"cell_type":"code","source":["# @title Your code here:\n","\n"],"metadata":{"id":"b0MgegPF19vy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Defining reconstruction model\n","\n","Now that our data is ready to be used, we define the model we will optimize to represent the scene. The first part is a SMPL-inspired **template model** controlling several features of the reconstructed dolphin, which are defined in the following cells (fine-grained details are beyond the scope of this tutorial)."],"metadata":{"id":"eNvMgKXmdl2y"}},{"cell_type":"code","source":["# @title Constants\n","\n","BASE_DOLPHIN_SIZE = 2.6 #m  Adult size of the indo-pacific bottlenose dolphin\n","BASE_MODEL_PARTS = 10   #   Number of bones in the original template model\n"],"metadata":{"id":"QT_SRYc0hJz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title [[Ignore]] Chumpy backend for template model loading\n","import cv2\n","import pickle\n","import chumpy as ch\n","import scipy.sparse as sp\n","from chumpy.ch import MatVecMult\n","use_python_3 = True\n","\n","class Rodrigues(ch.Ch):\n","    dterms = 'rt'\n","\n","    def compute_r(self):\n","        return cv2.Rodrigues(self.rt.r)[0]\n","\n","    def compute_dr_wrt(self, wrt):\n","        if wrt is self.rt:\n","            return cv2.Rodrigues(self.rt.r)[1].T\n","\n","def ischumpy(x): return hasattr(x, 'dterms')\n","\n","def verts_decorated(trans, pose,\n","    v_template, J, weights, kintree_table, bs_style, f,\n","    bs_type=None, posedirs=None, betas=None, shapedirs=None, want_Jtr=False):\n","\n","    for which in [trans, pose, v_template, weights, posedirs, betas, shapedirs]:\n","        if which is not None:\n","            assert ischumpy(which)\n","\n","    v = v_template\n","\n","    if shapedirs is not None:\n","        if betas is None:\n","            betas = ch.zeros(shapedirs.shape[-1])\n","        v_shaped = v + shapedirs.dot(betas)\n","    else:\n","        v_shaped = v\n","\n","    if posedirs is not None:\n","        v_posed = v_shaped + posedirs.dot(posemap(bs_type)(pose))\n","    else:\n","        v_posed = v_shaped\n","\n","    v = v_posed\n","\n","    if sp.issparse(J):\n","        regressor = J\n","        J_tmpx = MatVecMult(regressor, v_shaped[:,0])\n","        J_tmpy = MatVecMult(regressor, v_shaped[:,1])\n","        J_tmpz = MatVecMult(regressor, v_shaped[:,2])\n","        J = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T\n","    else:\n","        assert(ischumpy(J))\n","\n","    assert(bs_style=='lbs')\n","    result, Jtr = lbs_verts_core(pose, v, J, weights, kintree_table, want_Jtr=True, xp=ch)\n","\n","    tr = trans.reshape((1,3))\n","    result = result + tr\n","    Jtr = Jtr + tr\n","\n","    result.trans = trans\n","    result.f = f\n","    result.pose = pose\n","    result.v_template = v_template\n","    result.J = J\n","    result.weights = weights\n","    result.kintree_table = kintree_table\n","    result.bs_style = bs_style\n","    result.bs_type =bs_type\n","    if posedirs is not None:\n","        result.posedirs = posedirs\n","        result.v_posed = v_posed\n","    if shapedirs is not None:\n","        result.shapedirs = shapedirs\n","        result.betas = betas\n","        result.v_shaped = v_shaped\n","    if want_Jtr:\n","        result.J_transformed = Jtr\n","    return result\n","\n","def global_rigid_transformation(pose, J, kintree_table, xp):\n","    results = {}\n","    pose = pose.reshape((-1,3))\n","    id_to_col = {kintree_table[1,i] : i for i in range(kintree_table.shape[1])}\n","    parent = {i : id_to_col[kintree_table[0,i]] for i in range(1, kintree_table.shape[1])}\n","\n","    if xp == ch:\n","        rodrigues = lambda x : Rodrigues(x)\n","    else:\n","        import cv2\n","        rodrigues = lambda x : cv2.Rodrigues(x)[0]\n","\n","    with_zeros = lambda x : xp.vstack((x, xp.array([[0.0, 0.0, 0.0, 1.0]])))\n","    results[0] = with_zeros(xp.hstack((rodrigues(pose[0,:]), J[0,:].reshape((3,1)))))\n","\n","    for i in range(1, kintree_table.shape[1]):\n","        results[i] = results[parent[i]].dot(with_zeros(xp.hstack((\n","            rodrigues(pose[i,:]),\n","            ((J[i,:] - J[parent[i],:]).reshape((3,1)))\n","            ))))\n","\n","    pack = lambda x : xp.hstack([np.zeros((4, 3)), x.reshape((4,1))])\n","\n","    results = [results[i] for i in sorted(results.keys())]\n","    results_global = results\n","\n","    if True:\n","        results2 = [results[i] - (pack(\n","            results[i].dot(xp.concatenate( ( (J[i,:]), 0 ) )))\n","            ) for i in range(len(results))]\n","        results = results2\n","    result = xp.dstack(results)\n","    return result, results_global\n","\n","def lbs_verts_core(pose, v, J, weights, kintree_table, want_Jtr=False, xp=ch):\n","    A, A_global = global_rigid_transformation(pose, J, kintree_table, xp)\n","    T = A.dot(weights.T)\n","\n","    rest_shape_h = xp.vstack((v.T, np.ones((1, v.shape[0]))))\n","\n","    v =(T[:,0,:] * rest_shape_h[0, :].reshape((1, -1)) +\n","        T[:,1,:] * rest_shape_h[1, :].reshape((1, -1)) +\n","        T[:,2,:] * rest_shape_h[2, :].reshape((1, -1)) +\n","        T[:,3,:] * rest_shape_h[3, :].reshape((1, -1))).T\n","\n","    v = v[:,:3]\n","\n","    if not want_Jtr:\n","        return v\n","    Jtr = xp.vstack([g[:3,3] for g in A_global])\n","    return (v, Jtr)\n","\n","def verts_core(pose, v, J, weights, kintree_table, bs_style, want_Jtr=False, xp=ch):\n","\n","    if xp == ch:\n","        assert(hasattr(pose, 'dterms'))\n","        assert(hasattr(v, 'dterms'))\n","        assert(hasattr(J, 'dterms'))\n","        assert(hasattr(weights, 'dterms'))\n","    assert(bs_style=='lbs')\n","    result = lbs_verts_core(pose, v, J, weights, kintree_table, want_Jtr, xp)\n","\n","    return result\n","\n","def lrotmin(p):\n","    if isinstance(p, np.ndarray):\n","        p = p.ravel()[3:]\n","        return np.concatenate([(cv2.Rodrigues(np.array(pp))[0]-np.eye(3)).ravel() for pp in p.reshape((-1,3))]).ravel()\n","    if p.ndim != 2 or p.shape[1] != 3:\n","        p = p.reshape((-1,3))\n","    p = p[1:]\n","    return ch.concatenate([(Rodrigues(pp)-ch.eye(3)).ravel() for pp in p]).ravel()\n","\n","def posemap(s):\n","    if s == 'lrotmin':\n","        return lrotmin\n","    else:\n","        raise Exception('Unknown posemapping: %s' % (str(s),))\n","\n","def backwards_compatibility_replacements(dd):\n","\n","    # replacements\n","    if 'default_v' in dd:\n","        dd['v_template'] = dd['default_v']\n","        del dd['default_v']\n","    if 'template_v' in dd:\n","        dd['v_template'] = dd['template_v']\n","        del dd['template_v']\n","    if 'joint_regressor' in dd:\n","        dd['J_regressor'] = dd['joint_regressor']\n","        del dd['joint_regressor']\n","    if 'blendshapes' in dd:\n","        dd['posedirs'] = dd['blendshapes']\n","        del dd['blendshapes']\n","    if 'J' not in dd:\n","        dd['J'] = dd['joints']\n","        del dd['joints']\n","\n","    # defaults\n","    if 'bs_style' not in dd:\n","        dd['bs_style'] = 'lbs'\n","\n","def ready_arguments(fname_or_dict):\n","\n","    if not isinstance(fname_or_dict, dict):\n","        if use_python_3:\n","            dd = pickle.load(open(fname_or_dict, \"rb\"), encoding='latin1')\n","        else:\n","            dd = pickle.load(open(fname_or_dict, \"rb\"))\n","    else:\n","        dd = fname_or_dict\n","\n","    backwards_compatibility_replacements(dd)\n","\n","    want_shapemodel = 'shapedirs' in dd\n","    nposeparms = dd['kintree_table'].shape[1]*3\n","\n","    if 'trans' not in dd:\n","        dd['trans'] = np.zeros(3)\n","    if 'pose' not in dd:\n","        dd['pose'] = np.zeros(nposeparms)\n","    if 'shapedirs' in dd and 'betas' not in dd:\n","        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n","\n","    for s in ['v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs', 'betas', 'J']:\n","        if (s in dd) and not hasattr(dd[s], 'dterms'):\n","            dd[s] = ch.array(dd[s])\n","\n","    if want_shapemodel:\n","        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas'])+dd['v_template']\n","        v_shaped = dd['v_shaped']\n","        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:,0])\n","        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:,1])\n","        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:,2])\n","        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T\n","        dd['v_posed'] = v_shaped + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n","    else:\n","        dd['v_posed'] = dd['v_template'] + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n","\n","    return dd\n","\n","def load_model(fname_or_dict):\n","    dd = ready_arguments(fname_or_dict)\n","\n","    args = {\n","        'pose': dd['pose'],\n","        'v': dd['v_posed'],\n","        'J': dd['J'],\n","        'weights': dd['weights'],\n","        'kintree_table': dd['kintree_table'],\n","        'xp': ch,\n","        'want_Jtr': True,\n","        'bs_style': dd['bs_style']\n","    }\n","\n","    result, Jtr = verts_core(**args)\n","    result = result + dd['trans'].reshape((1,3))\n","    result.J_transformed = Jtr + dd['trans'].reshape((1,3))\n","\n","    for k, v in dd.items():\n","        setattr(result, k, v)\n","\n","    return result\n"],"metadata":{"cellView":"form","id":"4IESOD2Le-_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2\n","\n","We will now implement a crucial part of our articulated model: the [Rodrigues' rotation formula](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula). This operation is the backbone of our brand of linear blend skinning (LBS), a simple linear model for mesh animation. The goal is to efficiently map 3D angles in axis-angle format (i.e. rotation around x,y,z in radians) to rotation matrices that we can actually use for computation."],"metadata":{"id":"1mIEUkEv38kQ"}},{"cell_type":"code","source":["# @title Your code here:\n","\n","\n","def quat_to_rotmat(quat):\n","    \"\"\"Convert quaternion coefficients to rotation matrix.\n","    Args:\n","        quat: size = [B, 4] 4 <===>(w, x, y, z)\n","    Returns:\n","        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n","    \"\"\"\n","    pass\n","\n","\n","def batch_rodrigues(theta):\n","    \"\"\"Convert axis-angle representation to rotation matrix.\n","    Args:\n","        theta: size = [B, 3]\n","    Returns:\n","        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n","    \"\"\"\n","    pass"],"metadata":{"id":"f_xGQCvI4Wyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Template model"],"metadata":{"id":"HCO-eIZe45xV"}},{"cell_type":"code","source":["# @title Linear Blend Skinning\n","\n","class LBS:\n","    '''\n","    Implementation of linear blend skinning, with additional bone and scale\n","    Input:\n","        V (BN, V, 3): vertices to pose and shape\n","        pose (BN, J, 3, 3) or (BN, J, 3): pose in rot or axis-angle\n","        bone (BN, K): allow for direct change of relative joint distances\n","        scale (1): scale the whole kinematic tree\n","    '''\n","    def __init__(self, J_regressor, parents, weights, shapedirs, segmentation):\n","        self.n_joints = weights.shape[1]\n","        self.parents = parents\n","        self.weights = weights[None].float()\n","        self.shapedirs = shapedirs\n","        self.J_regressor = J_regressor\n","        self.seg_grouping = segmentation['grouping']\n","        self.v_labels = segmentation['v_labels']\n","        self.v_labels_coarse = segmentation['v_labels_coarse']\n","        self.n_groups = self.v_labels_coarse.max().item() + 1\n","\n","        self.gidx = []\n","        self.seg_idx_coarse = []\n","        for i in range(self.n_groups):\n","            self.gidx.append(torch.where(self.seg_grouping == i)[0].cpu().tolist())\n","            self.seg_idx_coarse.append(torch.where(self.v_labels_coarse == i)[0])\n","\n","        self.seg_idx = []\n","        self.parent_idx = []\n","        for i in range(self.n_joints):\n","            self.seg_idx.append(torch.where(self.v_labels == i)[0])\n","            self.parent_idx.append(torch.where(self.parents == i)[0].cpu().tolist())\n","\n","    def __call__(self, V, pose, scale, betas=None, to_rotmats=True):\n","        batch_size = pose.shape[0]\n","        device = pose.device\n","\n","        if betas is not None:\n","            V, J = self.apply_betas(V, betas)\n","        else:\n","            J = (self.J_regressor.unsqueeze(-1) * V.view(1, 1, -1, 3)).sum(dim=-2)\n","\n","        V = V.expand(batch_size, -1, -1) * scale\n","\n","        h_joints = F.pad(J.unsqueeze(-1), [0, 0, 0, 1], value=0)\n","        kin_tree = torch.cat([J[:, [0], :], J[:, 1:] - J[:, self.parents[1:]]], dim=1).unsqueeze(-1)\n","\n","        V = F.pad(V.unsqueeze(-1), [0, 0, 0, 1], value=1)\n","        kin_tree = scale * kin_tree.expand(batch_size, -1, -1, -1)\n","\n","        if to_rotmats:\n","            pose = batch_rodrigues(pose.view(-1, 3))\n","        pose = pose.view([batch_size, -1, 3, 3])\n","        T = torch.zeros([batch_size, self.n_joints, 4, 4]).float().to(device)\n","        T[:, :, -1, -1] = 1\n","        T[:, :, :3, :] = torch.cat([pose, kin_tree], dim=-1)\n","        T_rel = [T[:, 0]]\n","        for i in range(1, self.n_joints):\n","            T_rel.append(T_rel[self.parents[i]] @ T[:, i])\n","        T_rel = torch.stack(T_rel, dim=1)\n","        T_rel[:, :, :, [-1]] -= T_rel.clone() @ (h_joints * scale)\n","        T_ = self.weights @ T_rel.view(batch_size, self.n_joints, -1)\n","        T_ = T_.view(batch_size, -1, 4, 4)\n","        V = T_ @ V\n","\n","        return V[:, :, :3, 0]\n","\n","    def apply_betas(self, V, betas):\n","\n","        V_betas = V + ((self.shapedirs * betas.view(self.n_groups, 1, 1, 4)).sum(dim=-1))\n","        J = torch.zeros((self.n_groups, self.n_joints, 3), device=V.device)\n","        V_shaped = torch.zeros_like(V)\n","        for i in range(self.n_groups):\n","            seg_idx = self.seg_idx_coarse[i]\n","            V_shaped[:, seg_idx, :] = V_betas[i, seg_idx, :]\n","            J[i] = (self.J_regressor.unsqueeze(-1) * V_betas[i].view(1, 1, -1, 3)).sum(dim=-2)\n","\n","        # Fix this to obtain joint displacements\n","        Dv = torch.zeros((self.n_joints, 3), device=V.device)\n","        for seg in range(self.n_groups):\n","            gidx = self.gidx[seg]\n","            for p in gidx:\n","                if p > 0:\n","                    parent = self.parents[p]\n","                    # Look at where the joint would be in the shape of the parent\n","                    if self.seg_grouping[parent] == self.seg_grouping[p]:\n","                        dv = Dv[parent,:]\n","                    else:\n","                        dv = J[self.seg_grouping[parent], p, :] - J[self.seg_grouping[p], p, :]\n","                    Dv[p, :] = dv\n","                    idx = self.seg_idx[p]\n","                    V_shaped[:, idx, :] = V_shaped[:, idx, :] + dv\n","                    # find the children\n","                    idx = self.parent_idx[p]\n","\n","                    for i in idx:\n","                        # Add the displacement to the children's joints as I have translated the whole part\n","                        J[self.seg_grouping[p], i, :] += dv\n","\n","        J = (self.J_regressor.unsqueeze(-1) * V_shaped.view(1, 1, -1, 3)).sum(dim=-2)\n","        return V_shaped, J\n"],"metadata":{"id":"w3gR9ZEb3Pvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Template model definition\n","\n","import kaolin as kal\n","import pickle as pkl\n","from torch.nn import functional as F\n","\n","\n","class DolphinModel:\n","\n","    def __init__(self, data_dir: pathlib.Path, device=torch.device('cpu')):\n","\n","        self.device = device\n","        data_dir = pathlib.Path(MODEL_DIR)\n","\n","        self.vert2kpt = torch.from_numpy(np.load(data_dir / 'dol_verts2kp.npy')).to(device, dtype=torch.float)\n","\n","        model = load_model(data_dir / 'dolphin_model.pkl')\n","        # self.J = torch.from_numpy(model.J.r).to(device, dtype=torch.float).unsqueeze(0)\n","        self.J_regressor = torch.from_numpy(model.J_regressor.todense()).to(device, dtype=torch.float).unsqueeze(0)\n","        self.V = torch.from_numpy(model.v_template.r).to(device, dtype=torch.float).unsqueeze(0)\n","        self.F = torch.from_numpy(model.f).to(device, dtype=torch.long)\n","        self.weights = torch.from_numpy(model.weights.r).to(device, dtype=torch.float)\n","        self.kintree_table = torch.from_numpy(model.kintree_table).to(device, dtype=torch.long)\n","        self.parents = self.kintree_table[0]\n","\n","        kal_mesh = kal.io.obj.import_mesh(data_dir / 'dolphin_template.obj', with_materials=True)\n","        self.face_uvs = kal_mesh.face_uvs.to(device, dtype=torch.float).unsqueeze(0)\n","\n","        with open(data_dir / 'dolphin_seg_gloss.pkl', 'rb') as f:\n","            segmentation = pkl.load(f)\n","        vert_parts = torch.from_numpy(segmentation['v_labels']).to(device, dtype=torch.long)\n","\n","        with open(data_dir / 'dolphin_coarse_seg.pkl', 'rb') as f:\n","            coarse_parts = pkl.load(f)\n","        coarse_vert_parts = torch.from_numpy(coarse_parts['coarse_v_labels']).to(device, dtype=torch.long)\n","        coarse_parts_idx = coarse_parts['coarse_parts_idx']\n","        segmentation_grouping = -1 * torch.ones((BASE_MODEL_PARTS), dtype=int, device=self.device)\n","        for i in range(BASE_MODEL_PARTS):\n","            for j in range(len(coarse_parts_idx)):\n","                if i in coarse_parts_idx[j]:\n","                    segmentation_grouping[i] = j\n","\n","        shapedirs = torch.from_numpy(model.shapedirs.r).to(device, dtype=torch.float).unsqueeze(0)\n","        self.LBS = LBS(self.J_regressor, self.parents, self.weights, shapedirs, {\n","            'grouping': segmentation_grouping, 'v_labels': vert_parts, 'v_labels_coarse': coarse_vert_parts\n","        })\n","\n","        self.global_scale = BASE_DOLPHIN_SIZE / (self.V[0, :, 2].max() - self.V[0, :, 2].min())\n","        self.fwd_vector = torch.tensor([0.0, 0.0, 1.0], device=device)\n","\n","        self.bone_selection = torch.tensor([1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], device=device)\n","\n","    def __call__(self, global_pose, body_pose, translation, part_scales=None, pose2rot=True):\n","\n","        # concatenate bone and pose\n","        pose = torch.cat([global_pose, body_pose], dim=1) * self.bone_selection.view(1, -1, 1)\n","\n","        # LBS\n","        verts = self.LBS(self.V, pose, self.global_scale, betas=part_scales, to_rotmats=pose2rot)\n","\n","        # Final output after articulation\n","        output = {'vertices': verts + translation}\n","\n","        return output"],"metadata":{"id":"lfUhAdU1jsRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After the template model, it's time to define the set of variables we will optimize."],"metadata":{"id":"-wSBvc2UyQ_X"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","NUM_FRAMES = len(dataset)\n","INIT_ROT_UP = 0.0\n","INIT_ROT_FWD = 0.0\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","dolphin_model = DolphinModel(MODEL_DIR, device)\n","\n","# Create num_frames pose parameters\n","global_poses = nn.Parameter(torch.tensor([[[0.0, np.deg2rad(INIT_ROT_UP), np.deg2rad(INIT_ROT_FWD)]]], device=device).repeat(NUM_FRAMES, 1, 1), requires_grad=True)\n","joint_poses = nn.Parameter(torch.zeros([NUM_FRAMES, dolphin_model.LBS.n_groups, 3], device=device), requires_grad=True)\n","translations = nn.Parameter(torch.zeros([NUM_FRAMES, 1, 3], device=device), requires_grad=True)\n","\n","# Dolphin shape can't change over time\n","part_scales = nn.Parameter(torch.zeros((dolphin_model.LBS.n_groups, 4), requires_grad=True, device=device))\n","\n","# Create lighting parameters\n","azimuth = nn.Parameter(torch.full((1,), torch.pi / 2., device=device), requires_grad=True)\n","elevation = nn.Parameter(torch.full((1,), torch.pi / 2., device=device), requires_grad=True)\n","strength = nn.Parameter(torch.full((1,), 1.0, device=device), requires_grad=True)\n","angle = nn.Parameter(torch.full((1,), torch.pi / 2.0, device=device), requires_grad=True)\n","\n","# Create textures\n","dolphin_albedo = nn.Parameter(torch.full([1, 1, 512, 512], 0.5, device=device), requires_grad=True)\n","dolphin_specular_color = nn.Parameter(torch.full([3,], 1.0, device=device), requires_grad=False)\n","dolphin_roughness = nn.Parameter(torch.full([1,], 0.1, device=device), requires_grad=False)\n","\n","# Create water filter parameter\n","water_absorption = nn.Parameter(torch.tensor([1.0, 0.0, 0.0], device=device), requires_grad=True)\n"],"metadata":{"id":"JqcznJSwyRkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 3\n","\n","Due to our particular setting (we can assume the camera to be perfectly perpendicular to the object), we can employ the drone metadata and the SAM-2 masks to initialize the location of the dolphin in the scene, so that the dolphin object and the segmentation mask are intersecting at step 0."],"metadata":{"id":"_CzhtaDwfVoX"}},{"cell_type":"code","source":["import math\n","from torch.utils.data import DataLoader\n","from kaolin.render.camera.intrinsics import CameraFOV\n","\n","# Employs the normalized center of pixel coordinates of the segmentation masks to compute\n","# an initialization of each frame's global translation which guarantees alignment between\n","# the rendered shape and the segmentation mask.\n","\n","loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_batch)\n","for i, batch in tqdm(enumerate(loader), desc=\"Initializing translations...\"):\n","    # We have to invert the following flow of operations:\n","    # 1. extrinsics projection (easy)\n","    # 2. intrinsics projection (probably doable only in our special case)\n","    # 3. normalization (doable only in our special case)\n","    camera = batch['cam']\n","    camera_altitude = -camera.extrinsics.t[0, 2, 0:1]\n","    # We want the y (up) coord for this point in 3D world space to be 0\n","    image_plane_translation = batch['segm_center'][0]\n","    # We know that the input for the intrinsic projection is (in this specific setting):\n","    # [original_x, original_z, original_y - camera_altitude]\n","    # But since original_y = 0, we get [original_x, original_z, -camera_altitude]\n","    # Also for all cameras, the normalizing coordinate (w) is exactly -input_z=-camera_altitude\n","    # Therefore we begin with step 3:\n","    unnormalized_coords = image_plane_translation * camera_altitude\n","    # We now only need the 3rd coordinate to invert the intrinsic projection\n","    intrinsic_proj = camera.intrinsics.projection_matrix()\n","    z_transform = intrinsic_proj[0, 2, :]\n","    # We put unnormalized_coords in the first two spots but it doesn't really matter (in our case)\n","    # since z_proj_output has 0 in the first 2 components\n","    intrinsic_proj_input = torch.cat([unnormalized_coords, -camera_altitude, torch.ones([1], dtype=torch.float)], dim=-1)\n","    z_proj_output = (z_transform * intrinsic_proj_input).sum(dim=-1, keepdim=True)\n","    projection_output = torch.cat([unnormalized_coords, z_proj_output, camera_altitude], dim=-1)\n","    # And we invert step 2\n","    projection_input = (intrinsic_proj[0].inverse() @ projection_output)[:3]\n","    # And we invert step 1\n","    original_point = camera.extrinsics.R[0].T @ (projection_input - camera.extrinsics.t[0, :, 0])\n","    translations.data[i, ...] = original_point.unsqueeze(0).to(device)"],"metadata":{"id":"VX1YaMydg9jV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, for each frame $i$:\n","1. Obtain the initialized translation $T_i$\n","2. Use the camera object $C_i$ from the dataset to project it to pixel space\n","3. **Tracking**: Create a black image with the same resolution as the data. Then, find each pixel corresponding to some projection obtained at step (2). Color said pixels according to their temporal order and display the image."],"metadata":{"id":"t_8k-uVr-reu"}},{"cell_type":"code","source":["# @title Your code here:"],"metadata":{"id":"w43kMLUI_tND"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Fitting with differentiable rendering\n","\n","At this stage, we need to implement a simple differentiable render, which will allow us to:\n","1. Render the current scene parameters into an image\n","2. Compare the output with the data frames\n","3. Compute losses and backpropagate gradients\n","\n","We will use the DIB-R algorithm implemented in the NVIDIA kaolin library."],"metadata":{"id":"_MnvKE-Be6nW"}},{"cell_type":"code","source":["# @title [[Ignore]] Rendering helper functions\n","\n","@torch.jit.script\n","def _dot(a, b):\n","    \"\"\"Compute dot product of two tensors on the last axis.\"\"\"\n","    return torch.sum(a * b, dim=-1, keepdim=True)\n","\n","@torch.jit.script\n","def _ggx_v1(m2, nDotX):\n","    \"\"\"Helper for computing the Smith visibility term with Trowbridge-Reitz (GGX) distribution\"\"\"\n","    return 1. / (nDotX + torch.sqrt(m2 + (1. - m2) * nDotX * nDotX))\n","\n","\n","def sg_warp_specular_term_full_diff(amplitude, direction, sharpness, normal,\n","                                    roughness, view, spec_albedo):\n","    assert amplitude.ndim == 2 and amplitude.shape[-1]\n","    assert direction.shape == amplitude.shape\n","    assert sharpness.shape == amplitude.shape[:1]\n","    assert normal.ndim == 2 and normal.shape[-1] == 3\n","    assert roughness.shape == normal.shape[:1]\n","    assert view.shape == normal.shape\n","    assert spec_albedo.shape == normal.shape\n","    ndf_amplitude, ndf_direction, ndf_sharpness = kal.render.lighting.sg_distribution_term(\n","        normal, roughness)\n","    ndf_amplitude, ndf_direction, ndf_sharpness = kal.render.lighting.sg_warp_distribution(\n","        ndf_amplitude, ndf_direction, ndf_sharpness, view)\n","    ndl = torch.clamp(_dot(normal, ndf_direction), min=0., max=1.)\n","    ndv = torch.clamp(_dot(normal, view), min=0., max=1.)\n","    h = ndf_direction + view\n","    h = h / torch.sqrt(_dot(h, h))\n","    ldh = torch.clamp(_dot(ndf_direction, h), min=0., max=1.)\n","\n","    output = kal.render.lighting.sg.unbatched_reduced_sg_inner_product(\n","        ndf_amplitude, ndf_direction, ndf_sharpness, amplitude, direction, sharpness)\n","    m2 = (roughness * roughness).unsqueeze(-1)\n","    output = output * (_ggx_v1(m2, ndl) * _ggx_v1(m2, ndv))\n","    output = output * kal.render.lighting.fresnel(ldh, spec_albedo)\n","    output = output * ndl\n","    return torch.clamp(output, min=0.)"],"metadata":{"cellView":"form","id":"mV7jXw7eiH5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Constants\n","RENDER_RESOLUTION = dataset.data_resolution\n","SIGMAINV = 100000   # Defines sharpness of the optimiz-able band of pixels around object\n","BOXLEN = 0.01       # Defines box of influence over pixels for each mesh element\n","KNUM = 40           # Maximum number of mesh elements contributing to each pixel"],"metadata":{"id":"MtBJAW5Ne_fB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Dolphin skinning\n","\n","def dolphin_skinning(t):\n","    global_batch = global_poses[t, ...]\n","    joint_batch = joint_poses[t, ...].view(-1, dolphin_model.LBS.n_groups, 3)[:, dolphin_model.LBS.seg_grouping, :][:, 1:, :]  # filter out spine joint\n","    trans_batch = translations[t, ...]\n","    parts_batch = part_scales\n","\n","    return dolphin_model(global_batch, joint_batch, trans_batch, part_scales=parts_batch)['vertices']\n"],"metadata":{"id":"6qicaIU4yfAb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 4: Basic rendering\n","\n","This function is the core routine of our differentiable renderer. Complete the code below following this template:\n","\n","1. $V_e$ = Apply the extrinsic camera transform to the mesh vertices, projecting them to \"camera space\". *(Hint: `cam.extrinsics.transform`*)\n","2. $V_i$ = Apply the intrinsic camera transform to $V_e$. The vertices are now projected to \"render space\", so we only care about its first two coordinates.\n","3. $F_e, F_i$ = Index the outputs of points 1 and 2 using the mesh's face indices. *(Hint: `kaolin.ops.mesh.index_vertices_by_faces`)*\n","4. $F_n$ = Compute face normals in camera space. *(Hint: `kaolin.ops.mesh.face_normals`)*\n","5. We need to define which mesh features we want DIB-R to interpolate. For texturing (later), we will interpolate face uvs and normals, and we will pass a tensor of 1s to obtain a hard rendering mask. We also want to interpolate the sea level height of each rendered mesh point. *(Hints: the up axis is y, the 2nd coordinate of vertex matrices. Also, the value needs to be per-face.)*\n","6. `rasterize` needs the following information:\n","    1. Depth (z coordinate) of face vertices in camera space\n","    2. Face vertices in render space\n","    3. Which faces are \"looking\" at the camera *(Hint: consider the z coordinate of their normals)*\n","7. `dibr_soft_mask` needs the face vertices in render space to compute the soft mask."],"metadata":{"id":"5lgrEOzOBOpY"}},{"cell_type":"code","source":["# @title Your code here:\n","\n","def dibr_rendering(verts, cam):\n","    batch_size = verts.shape[0]\n","    mesh = kal.rep.SurfaceMesh(verts, dolphin_model.F, face_uvs=dolphin_model.face_uvs.repeat(batch_size, 1, 1, 1))\n","    mesh.face_uvs[..., 1] = 1 - mesh.face_uvs[..., 1]\n","\n","    ???\n","\n","    face_attributes = [\n","        mesh.face_uvs, mesh.face_normals, ???,\n","        torch.ones((batch_size, mesh.faces.shape[0], 3, 1), device=face_vertices_image.device)\n","    ]\n","\n","    image_features, rendered_faces = kal.render.mesh.rasterize(RENDER_RESOLUTION[1], RENDER_RESOLUTION[0],\n","                                                               ???, ???, face_attributes, valid_faces=???, backend='cuda')\n","    soft_mask = kal.render.mesh.dibr_soft_mask(???, rendered_faces,\n","                                               SIGMAINV, BOXLEN, KNUM)[..., None]\n","\n","    return image_features, soft_mask\n"],"metadata":{"id":"b3a8LMngywfa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Completing the renderer"],"metadata":{"id":"zB8hpTWpBaQh"}},{"cell_type":"code","source":["# @title Texturing\n","# @markdown The texture coordinates and hard mask we need here are the first and last face attributes we interpolated with DIB-R!\n","\n","def texturing(texture_coords, hard_mask):\n","    batch_size = texture_coords.shape[0]\n","    image = kal.render.mesh.texture_mapping(texture_coords, dolphin_albedo.repeat(batch_size, 3, 1, 1), mode='bilinear')\n","    return image * hard_mask"],"metadata":{"id":"b0L-edMHzdFh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 5: Lighting effects\n","\n","We will implement a simple exponential, frequency-based filtering on the image. This works as a post-processing and will remove specific frequencies from the image pixels depending on the sea depth of the rendered object. The filter frequencies are one of the parameters we optimize, `water_absorption`.\n","\n","For pixels $ij$ where the sea level height is below 0, we want to modify the image color as: $$I_{ij} \\leftarrow I_{ij} \\cdot \\exp(w \\cdot l_{ij}) $$\n","Where $w$ indicates the `water_absorption` parameter and $l_{ij}$ is the sea level height at pixel $ij$."],"metadata":{"id":"gGaW9SwSI39s"}},{"cell_type":"code","source":["# @title Your code here\n","# @markdown The image normals and sea level height we need here are the 2nd and 3rd face attributes we interpolated with DIB-R!\n","\n","def lighting(cam, hard_mask, image, image_normals, sl_height_mapped):\n","    bool_mask = hard_mask.bool()[..., 0]\n","    direction = torch.cat(kal.ops.coords.spherical2cartesian(azimuth, elevation), dim=-1).unsqueeze(0)\n","    sg_sun = kal.render.lighting.SgLightingParameters.from_sun(direction, strength, angle)\n","    spec_albedo = hard_mask * dolphin_specular_color.view(1, 1, 1, -1)\n","    roughness = hard_mask * dolphin_roughness.view(1, 1, 1, -1)\n","    rays_d = -torch.stack(\n","        [kal.render.camera.generate_pinhole_rays(c)[1].reshape(*RENDER_RESOLUTION[::-1], 3) for c in cam], dim=0)\n","    diffuse_effect = kal.render.lighting.sg_diffuse_fitted(\n","        sg_sun.amplitude,\n","        direction,\n","        sg_sun.sharpness,\n","        image_normals[bool_mask, :],\n","        image[bool_mask, :]\n","    )\n","    specular_effect = sg_warp_specular_term_full_diff(\n","        sg_sun.amplitude,\n","        direction,\n","        sg_sun.sharpness,\n","        image_normals[bool_mask, :],\n","        roughness[bool_mask, :][..., 0],\n","        rays_d[bool_mask, :],\n","        spec_albedo[bool_mask, :]\n","    )\n","    image[bool_mask, :] = diffuse_effect + specular_effect\n","\n","    # Apply water volume absorption:\n","    ???\n","\n","    return image"],"metadata":{"id":"f0SKT7cjzs9x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 6: Rendering function\n","\n","Now, put all these sub-routines together to obtain a complete renderer. Here is a pseudo-code:\n","\n","1. Deform the template vertices\n","2. Run the core differentiable rendering procedure\n","3. Apply texturing\n","4. Apply lighting"],"metadata":{"id":"rmqRRTOYBkfg"}},{"cell_type":"code","source":["# @title Your code here:\n","\n","from typing import List\n","\n","def render(t: List[int], cam: kal.render.camera.Camera):\n","    ???\n","    return image, hard_mask, soft_mask"],"metadata":{"id":"qDig0DEyyL77"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Loss function design\n","\n","Our loss function will be composed of multiple terms, which we will now implement. Remember that we can only optimize the rendered image, the soft mask (both only in the pixels where the parametrized object was hit), and the scene parameters directly (*e.g.* for regularization)."],"metadata":{"id":"mR6GTeKR77qd"}},{"cell_type":"markdown","source":["## Data fidelity terms"],"metadata":{"id":"qjxjpNf4jiZi"}},{"cell_type":"code","source":["def photometric_loss(render, original, hard_mask):\n","    bool_mask = hard_mask.bool()[..., 0]\n","    return F.mse_loss(render[bool_mask, :], original[bool_mask, :])\n","\n","def iou_loss(silhouette, segmentation):\n","    return kal.metrics.render.mask_iou(silhouette[..., 0], segmentation[..., 0])"],"metadata":{"id":"VktsUywI7-Om"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parameter distribution priors"],"metadata":{"id":"BTRpEK1Oj8uC"}},{"cell_type":"code","source":["def scale_prior(scale_params, unseen_axis):\n","    global_prior = (scale_params ** 2).mean()\n","    axes_scaling = scale_params[..., 1:]\n","    x_scale, y_scale, z_scale = axes_scaling.split(1, dim=-1)\n","    # Make unseen axis scaling similar to that of other axes\n","    if unseen_axis == 'x':\n","        per_axis_prior = (\n","            ((x_scale - y_scale.detach()) ** 2).sum(dim=-1) +\n","            ((x_scale - z_scale.detach()) ** 2).sum(dim=-1)\n","        ).mean()\n","    elif unseen_axis == 'y':\n","        per_axis_prior = (\n","            ((x_scale.detach() - y_scale) ** 2).sum(dim=-1) +\n","            ((y_scale - z_scale.detach()) ** 2).sum(dim=-1)\n","        ).mean()\n","    part_scales = scale_params[dolphin_model.LBS.seg_grouping]\n","    scale_s = ((part_scales[1:] - part_scales[dolphin_model.LBS.parents[1:]]) ** 2).mean()\n","    return {'global': global_prior, 'axes': per_axis_prior, 'smoothness': scale_s}\n","\n","bone_prior_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0], device=device)  # Only one value for lower fins and tails (symmetry)\n","\n","def pose_prior(joint_poses):\n","    return ((joint_poses ** 2) * bone_prior_weights.view(1, -1, 1)).mean()"],"metadata":{"id":"YZ_T6NW4kLoR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Temporal priors"],"metadata":{"id":"9IAMBZQKk8CS"}},{"cell_type":"code","source":["def smoothness_prior(global_poses, joint_poses, translations):\n","    trans_s = ((translations[1:, ...] - translations[:-1, ...]) ** 2).mean()\n","    global_s = ((global_poses[1:, ...] - global_poses[:-1, ...]) ** 2).mean()\n","    joint_s = ((joint_poses[1:, ...] - joint_poses[:-1, ...]) ** 2).mean()\n","    return {'transl': trans_s, 'global': global_s, 'joint': joint_s}"],"metadata":{"id":"p6HBIL_Mk7m_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Putting it all together:"],"metadata":{"id":"FzExc_PwlbuL"}},{"cell_type":"code","source":["loss_weights = {\n","    'mask': 1.0,\n","    'scale': {\n","        'global': 0.001,\n","        'axes': 0.1,\n","        'smoothness': 5.0\n","    },\n","    'pose': 2.0,\n","    'smoothness': {\n","        'translations': 500.0,\n","        'global_poses': 0.0,\n","        'joint_poses': 500.0\n","    }\n","}\n","\n","def compute_loss(render, silhouette, original, segmentation, hard_mask,\n","         scale_params, global_poses, joint_poses, translations):\n","    photometric = photometric_loss(render, original, hard_mask)\n","    mask = loss_weights['mask'] * iou_loss(silhouette, segmentation)\n","    pose = loss_weights['pose'] * pose_prior(joint_poses)\n","\n","    scale = scale_prior(scale_params, 'y')\n","    scale_global = loss_weights['scale']['global'] * scale['global']\n","    scale_axes = loss_weights['scale']['axes'] * scale['axes']\n","    scale_smooth = loss_weights['scale']['smoothness'] * scale['smoothness']\n","\n","    smoothness = smoothness_prior(global_poses, joint_poses, translations)\n","    smooth_translations = loss_weights['smoothness']['translations'] * smoothness['transl']\n","    smooth_gposes = loss_weights['smoothness']['global_poses'] * smoothness['global']\n","    smooth_jposes = loss_weights['smoothness']['joint_poses'] * smoothness['joint']\n","\n","    return photometric + mask + pose + \\\n","      scale_global + scale_axes + scale_smooth + \\\n","      smooth_translations + smooth_gposes + smooth_jposes\n"],"metadata":{"id":"MAeuajN-ldkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimization loop\n","\n","We now design a simple gradient-descent based optimization loop to fit the parameters to our data."],"metadata":{"id":"T0CR-7y3nIYK"}},{"cell_type":"code","source":["# @title [[Ignore]] Optimization helper functions\n","\n","def dict_to_device(data, device):\n","    return {k: (data[k].to(device) if hasattr(data[k], 'to') else data[k]) for k in data.keys()}\n","\n","def safe_clamp(param, min=None, max=None):\n","    param.data.clamp_(min=min, max=max)"],"metadata":{"cellView":"form","id":"W6UE4T3eqOSB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Optimization loop\n","\n","Complete the optimization loop below using the rendering and loss functions we designed in the previous sections."],"metadata":{"id":"b9vKhXdALiRP"}},{"cell_type":"code","source":["# @title Your code here:\n","\n","STEPS = 50\n","BATCH_SIZE = 7\n","LRS = {\n","    'transform': 0.01,\n","    'pose': 0.01,\n","    'texture': 0.001,\n","    'lighting': 0.01\n","}\n","\n","import torch.optim as opt\n","\n","\n","optimizer = opt.Adam([\n","    {'params': [translations, part_scales], 'lr': LRS['transform']},\n","    {'params': [global_poses, joint_poses], 'lr': LRS['pose']},\n","    {'params': [dolphin_albedo, water_absorption], 'lr': LRS['texture']},\n","    {'params': [azimuth, elevation, strength, angle], 'lr': LRS['lighting']}\n","])\n","\n","loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","\n","outer_loop = tqdm(range(STEPS))\n","for i in outer_loop:\n","    step_losses = []\n","    for frames in tqdm(loader, leave=False):\n","        frames = dict_to_device(frames, device)\n","\n","        optimizer.zero_grad()\n","\n","        ???\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        safe_clamp(water_absorption, 0, 1)\n","        safe_clamp(dolphin_albedo, 0, 1)\n","        safe_clamp(strength, min=0.0)\n","        safe_clamp(azimuth, min=0.0, max=2*torch.pi)\n","        safe_clamp(angle, min=0.0, max=2*torch.pi)\n","        safe_clamp(elevation, min=0.0, max=2*torch.pi)\n","\n","        step_losses.append(loss.item())\n","\n","    outer_loop.set_description(f'Loss: {np.array(step_losses).mean()}')"],"metadata":{"id":"jItXkX4Los-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's save the trained model:\n","\n","torch.save(\n","    {\n","        'translations': translations,\n","        'part_scales': part_scales,\n","        'global_poses': global_poses,\n","        'joint_poses': joint_poses,\n","        'dolphin_albedo': dolphin_albedo,\n","        'water_absorption': water_absorption,\n","        'azimuth': azimuth,\n","        'elevation': elevation,\n","        'strength': strength,\n","        'angle': angle\n","    },\n","    f'{ROOT}/dolphin_model_fit.pth'\n",")"],"metadata":{"id":"AS2wBKu8h6uy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizing results\n","\n","After optimization, we will combine 3D and 2D visualizations to evaluate how well our reconstruction algorithm performed."],"metadata":{"id":"wgtKJpIhr88l"}},{"cell_type":"code","source":["import plotly.figure_factory as ff\n","\n","PICK_FRAME = 100  # Something in [0, 199]\n","data = dict_to_device(collate_batch([dataset[PICK_FRAME]]), device)\n","\n","with torch.no_grad():\n","    out_render, _, silhouette = render(data['t'], data['cam'])\n","    original, segmentation = data['original'], data['segmented']\n","\n","    global_batch = global_poses[data['t'], ...]\n","    joint_batch = joint_poses[data['t'], ...].view(-1, dolphin_model.LBS.n_groups, 3)[:, dolphin_model.LBS.seg_grouping, :][:, 1:, :]  # filter out spine joint\n","    trans_batch = translations[data['t'], ...]\n","    parts_batch = part_scales\n","    skinned_verts = dolphin_model(global_batch, joint_batch, trans_batch, part_scales=parts_batch)['vertices']\n","    dolphin_mesh = [skinned_verts[0, ...].cpu().numpy(), dolphin_model.F.cpu().numpy()]\n","\n","dolphin_verts, dolphin_faces = dolphin_mesh\n","fig = ff.create_trisurf(x=dolphin_verts[:, 0], y=dolphin_verts[:, 2], z=dolphin_verts[:, 1],\n","                        simplices=dolphin_faces)\n","fig.update_scenes(xaxis_range=[-2, 2], yaxis_range=[-2, 2], zaxis_range=[-3, 1])\n","fig.show()"],"metadata":{"id":"9uKphyuNsPXD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 8: Output video\n","\n","Let's combine the results with the input data to obtain an informative short video showing the output of our method.\n","\n","Iterate over all frames in the dataset in order and query our optimized model. For each frame, stack the original frame, segmentation, render and soft mask in the following way:\n","```\n","original frame | SAM-2 segmentation\n","---------------|-------------------\n","render         | DIB-r soft mask\n","```\n","For the render, to avoid a black background, let's do some compositing on the original frame: simply copy the original frame, and for all pixels where the DIB-R soft mask is greater than 0.5, replace the color of the original frame with the rendered one at the same pixel.\n","\n","Once you're done, download the video and take a look!"],"metadata":{"id":"j9NdlSydMCRT"}},{"cell_type":"code","source":["import cv2\n","\n","height, width = dataset.data_resolution[::-1]\n","video = cv2.VideoWriter(f'{ROOT}/dolphin_fit.avi', cv2.VideoWriter_fourcc(*'XVID'), 50.0, (width * 2, height * 2))\n","\n","loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_batch)\n","for data in loader:\n","\n","    ???\n","\n","    # frame has to be a numpy array with [0;1] values and shape (2 * height, 2 * width, 3)\n","    video.write((frame * 255).astype(np.uint8))\n","\n","cv2.destroyAllWindows()\n","video.release()\n"],"metadata":{"id":"9CpTjZtpqpO9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whoops, something went wrong: it's a known problem of working with the axis-angle representation, known as **gimbal lock**. Let's look for temporal discontinuities in the global orientation of the dolphin:"],"metadata":{"id":"B3Vgj5vkLCm8"}},{"cell_type":"code","source":["plt.plot(np.rad2deg(global_poses[:, 0, 0].cpu().detach().numpy()))\n","plt.plot(np.rad2deg(global_poses[:, 0, 1].cpu().detach().numpy()))\n","plt.plot(np.rad2deg(global_poses[:, 0, 2].cpu().detach().numpy()))\n","plt.title(\"Gimbal lock\")\n","plt.show()"],"metadata":{"id":"vmrNRUGPxmLr"},"execution_count":null,"outputs":[]}]}
